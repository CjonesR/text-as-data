

# Text as Data

Pr. John Laudun / HLG 356 / laudun@louisiana.edu



The past decade has witnessed an explosion of data produced by just about everything and everyone everywhere. From individuals posting on Twitter or Reddit to governmental agencies and non-governmental organizations making data available either through documents or APIs, to the digitization of vast amounts of historical archives and administrative records, these troves of data offer analysts the ability to understand, and perhaps address, a range of  pressing problems within communities, market sectors, and industries. A lot of this data is in the form of text, messy and unstructured texts. Collecting and analyzing such data presents unique challenges. Fortunately, major advances in data science, natural language processing, discourse analysis, and computational text studies has offered analysts a number of useful footholds from which to launch their explorations. 

## What is text analytics?

Text analytics, also known as text mining, is the process of analyzing and extracting insights from textual data. It often uses a wide range of ideas, techniques, and methods that are known as natural language processing (NLP). Like other forms of analytics, text analytics involves using statistical and computational concepts and methods to identify and extract relevant information from unstructured text data. In many cases, the amount of data can be quite large, but it need not be — to be clear, while many methods for dealing with human linguistic expression remain the same, there are techniques that work better at either end of the size of a data set (also known as a corpus). 

Text analytics is important because it allows individuals and organizations to gain valuable insights from unstructured data sources such as social media posts, customer reviews, emails, and other forms of textual data. By analyzing these data sources, organizations can identify trends, sentiment, and customer preferences, which can inform decision-making, product development, marketing strategies, and more. At large end of the spectrum, models based on a millions of texts can generate, some would argue, Turing-complete blocks of text. (One of the preceding sentences  was, in fact, generated by a large language model, OpenAI.)

## What are you going to learn?

This course  will provide students with an overview of popular techniques for  collecting, processing, and analyzing text-based data—including web scraping, mining data from application programming interfaces (APIs), topic modeling, text networks, and advanced text classifiers.

In this course, you will have the opportunity to explore various kinds of text analysis: from gathering, preprocessing, and vectoring a collection, or corpus, of texts into high-dimensional feature representations; to performing document classification and topic modeling; to extracting key phrases, named entities, and graph structures; to sentiment analysis and other forms of controlled vocabulary application. Much of the work will be hands-on, working first with data sets provided by the course but with the goal of you developing a data set, and thus insights all your own. Final outputs will vary by student interest and appropriateness to the data, but can include a rich set of visualizations, a written report, or a chat bot. 

## How are you going to learn?

Few people learn things exactly the same way and at exactly the same pace. Sometimes hearing something once is enough for you; sometimes you need to see it multiple times. Sometimes you need to read it instead of hear it. Sometimes you need to see it. But most importantly, you need to do it. This course assumes a basic familiarity with Python and that you are capable of setting up, maintaining, and updating a Python environment. For those who need a refreshers, there will be a crash course on mini conda and the importance of maintaining virtual environments at the start of the course. Much of the course will proceed through either Jupyter notebooks provided to you either to complete or to use as a basis for your own work. Those will be found in the class GitHub repository, which is also where you are going to turn in assignments — please note even written assignments will be turned in via GitHub and we will be using plain text formatted in markdown for much of our writing!



## Resources

There is a list of supplemental material — texts, audio, video, interactive — under the resources section.

In order to give participants as many options as possible to learn, the following items are offered:

Jacob Perkins, author of *Python 3 Text Processing with NLTK 3 Cookbook*, has demos on stemming and lemmatization, sentiment analysis, tagging and chunk extraction, and phrase extraction and named entity recognition. [URL](http://text-processing.com).