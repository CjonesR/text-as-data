{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6247efaa-61a8-4c85-89d8-46682735f2fb",
   "metadata": {},
   "source": [
    "# Hayden's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c5e07-bf02-4a71-b4bf-ed24b283786a",
   "metadata": {},
   "source": [
    "I've reduced the number of imports here to those you might actually need. You will need either the NonnegativeMatrixFactorization or the LatentDiricheletAllocation for topic modeling. (Go back to those notebooks for more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b57b64-c56e-4ce9-87c7-d17454eaa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45978c0a-ab6f-46bf-a6a8-4601bf00156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "films = []\n",
    "for p in Path(\"../queue/Mystery\").glob('*.txt'):\n",
    "    with open(p, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        film = f.read()\n",
    "        films.append(film)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b1413-e913-4b76-a730-d76462c649f4",
   "metadata": {},
   "source": [
    "Instead of two separate loops through the films, one to tokenize and one to break the tokens into three groups, I am combining those two tasks into your **`film_chunker`** function. It's completely fine to do it in two steps. I am reducing it to one for the ease of readability (for me)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4952736d-6a75-445d-9f36-09a800a9fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def film_chunker(film):\n",
    "    ''' takes a filmscript as a string and tokenizes it and\n",
    "        returns three lists of tokens as approximate stand-ins\n",
    "        for Acts I, II, and III in the Hollywood formula\n",
    "    '''\n",
    "    # Tokenize the string\n",
    "    tokenized_film = word_tokenize(film)\n",
    "\n",
    "    # Break the list into three lists:\n",
    "    ## Set our chunk sizes\n",
    "    chunk1_size = 0.25\n",
    "    chunk2_size = 0.50\n",
    "    chunk3_size = 0.25\n",
    "\n",
    "    # Calculate the sizes of each chunk based on the length of the file\n",
    "    total_length = len(tokenized_film)\n",
    "    chunk1_end = int(total_length * chunk1_size)\n",
    "    chunk2_end = int(total_length * (chunk1_size + chunk2_size))\n",
    "\n",
    "    # Divide the file into three chunks\n",
    "    chunk1 = tokenized_film[:chunk1_end]\n",
    "    chunk2 = tokenized_film[chunk1_end:chunk2_end]\n",
    "    chunk3 = tokenized_film[chunk2_end:]\n",
    "\n",
    "    # Store the chunks for this film in a tuple\n",
    "    chunks = (chunk1, chunk2, chunk3)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d4e1d-ca52-4a46-b62a-e3e5f6bd5012",
   "metadata": {},
   "source": [
    "The script above now handles only one string (film) at a time and returns a tuple consisting of three lists of tokenized words. E.g.:\n",
    "```\n",
    "chunks = ( ['word', 'word', 'word'], ['word', 'word', 'word'], ['word', 'word'] )\n",
    "```\n",
    "What the code below does is run each film through the film chunker and return the tuple, and, since it's running through a list of strings, it returns a list of tuples. \n",
    "```\n",
    "chunked_films = [ ([], [], []), ([], [], []), ([], [], []), ... ]\n",
    "```\n",
    "What's below is a list comprehension, which is simply a more compact form of a for loop that doesn't require you to create an empty list first and then append it. In other words, you could do the same thing with the following:\n",
    "```python\n",
    "chunked_films =[]\n",
    "for film in films:\n",
    "    chunks = film_chunker(film)\n",
    "    chunked_films.append(chunks)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aeeccb5-e48c-446a-aee6-9e9754226e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 63.5 ms, total: 18.2 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chunked_films = [ film_chunker(film) for film in films ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839de38-3ada-42e0-b909-96b1e41bb469",
   "metadata": {},
   "source": [
    "To see the nested structure, you can keep slicing -- which also reveals how you can access each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208dc388-c941-4cfd-82f6-e5102191fe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# What's the top level structure\n",
    "print(type(chunked_films))\n",
    "\n",
    "# Okay, it's a list of:\n",
    "print(type(chunked_films[0]))\n",
    "\n",
    "# And each tuple is made up of:\n",
    "print(type(chunked_films[0][0]))\n",
    "\n",
    "# And the bottom turtle is:\n",
    "print(type(chunked_films[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5bcae-747a-4377-a801-80deb740e13e",
   "metadata": {},
   "source": [
    "I think the simplest way to re-package things is with a `for` loop. Some might argue that it's not very compact, but it gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd759593-f253-4caa-92ca-445815022572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88 µs, sys: 162 µs, total: 250 µs\n",
      "Wall time: 251 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "act1s = []\n",
    "act2s = []\n",
    "act3s = []\n",
    "for item in chunked_films:\n",
    "    act1s.append(item[0])\n",
    "    act2s.append(item[1])\n",
    "    act3s.append(item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a8f2512-39e4-4c8a-a64a-9e26bb04a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first film in our list is         RED RIDING HOOD\n",
      "\n",
      "\n",
      "It's 34214 words long.\n",
      "The first act is 8553.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first film in our list is {films[0][20:45]}\")\n",
    "print(f\"It's {len(word_tokenize(films[0]))} words long.\")\n",
    "print(f\"The first act is {len(act1s[0])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73d7bd-7a69-4c4b-b29e-32ae4119ef12",
   "metadata": {},
   "source": [
    "That number looks to be about a quarter of the overall length, so we appear to be in business!\n",
    "\n",
    "Now, treat `act1s` as a collection of documents and topic model them. The same goes for `act2s` and `act3s`. (You may need to join the lists into strings in order to feed them into the CountVectorizer, if you use LDA, or the TfidfVectorizer if you use NMF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
