{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Texts and Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Update:</b> The string method <code>lower()</code> has been added to the tokenization process.\n",
    "</div>\n",
    "\n",
    "*If you want to know how to get these colored \"callout\" boxes, see [IBM's markdown guide](https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Open and read the file to create a string object\n",
    "mdg_string = open('../data/mdg.txt', 'r').read()\n",
    "# Create a list of substrings, aka words\n",
    "mdg_words = nltk.tokenize.word_tokenize(mdg_string.lower())\n",
    "# Create the NLTK Text object\n",
    "mdg = nltk.Text(mdg_words)\n",
    "\n",
    "# Repeat for \"Heart of Darkness\"\n",
    "hod_string = open('../data/hod.txt', 'r').read()\n",
    "hod_words = nltk.tokenize.word_tokenize(hod_string.lower())\n",
    "hod = nltk.Text(hod_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to create a frequency list. Three are below: using built-in functions, using the `nltk`, and using `pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a dictionary\n",
    "mdg_dict = {}\n",
    "for word in mdg_words:\n",
    "    try:\n",
    "        mdg_dict[word] += 1\n",
    "    except: \n",
    "        mdg_dict[word] = 1\n",
    "\n",
    "# When in doubt print something out\n",
    "print(f\"mdg_dict is a {type(mdg_dict)} of {len(mdg_dict)} entries (tokens).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries are key, value pairs.\n",
    "# To retrieve a value, enter the key:\n",
    "mdg_dict[\"hunter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowing this, you can actually get the most frequent tokens. \n",
    "# (This is not suggested as it's difficult to read.)\n",
    "# What would you change to get the least frequent tokens?\n",
    "for word in sorted(mdg_dict, key=mdg_dict.get, reverse=True)[0:10]:\n",
    "    print(word, mdg_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas\n",
    "\n",
    "This is my preferred way, for a variety of reasons, and I'm going to show it to you quickly and without much explanation just for reference before proceeding to walk through the `nltk` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Then use my preferred way to turn a string of words into a list of words\n",
    "words = re.sub(\"[^a-zA-Z']\",\" \", mdg_string).lower().split()\n",
    "\n",
    "# Then create a pandas series\n",
    "mdg_series = pd.Series(words)\n",
    "\n",
    "# pandas series are a particular data structure\n",
    "mdg_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_counts = mdg_series.value_counts()\n",
    "print(mdg_counts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas makes certain kinds of graphing very easy\n",
    "mdg_counts.iloc[0:49].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the niceties of working with pandas\n",
    "mdg_counts.to_csv(\"mdg-counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways forward: using only NLTK functions or, in the second cell below, combining using preferred word list with the NLTK's `FreqDist` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()\n",
    "for sentence in nltk.tokenize.sent_tokenize(mdg_string):\n",
    "    for word in nltk.tokenize.word_tokenize(sentence):\n",
    "        fdist[word] += 1\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdg_dist = nltk.FreqDist()\n",
    "for word in words:\n",
    "    mdg_dist[word] +=1\n",
    "\n",
    "mdg_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ask Python what kind of data structure freq_dist is,\n",
    "# you'll get a rather unhelpful response, but LOOK ABOVE. What do you see?\n",
    "type(mdg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can work with freq_dist like any list of tuples\n",
    "\n",
    "for word, frequency in freq_dist.most_common(10):\n",
    "    print(f\"{word}:  {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_dist comes with a lot of functionality\n",
    "# (See Table 3.1 in Chapter 1 of the NLTK book for more ideas.)\n",
    "\n",
    "mdg_dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Texts, More Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div (text):\n",
    "    lexdiv =len(set(text)) / len(text)\n",
    "    return lexdiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lexical diversity of \\\"The Most Dangerous Game\\\" is {lex_div(mdg):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lexical diversity of \\\"Heart of Darkness\\\" is {lex_div(hod):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div(a_file):\n",
    "    the_string =  open(a_file, 'r').read()\n",
    "    the_words = re.sub(\"[^a-zA-Z']\",\" \", the_string).lower().split()\n",
    "    lexdiv = len (set (the_words)) / len (the_words)\n",
    "    return lexdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "\n",
    "for i in data:\n",
    "    the_file = \"../data/1924/texts/\"+i+\".txt\"\n",
    "    lexdiv = lex_div(the_file)\n",
    "    print(f\"{i}: {lexdiv:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hmmm* ... that's quite a range. Referring to the lexical diversities for \"The Most Dangerous Game\" and _Heart of Darkness_, what do you think is at work there? \n",
    "\n",
    "What happens if we add a text as a data point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = \"../data/hamlet.txt\"\n",
    "lex_div(hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Your turn:</b> Write code that explores the possible dimension in play here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequencies\n",
    "\n",
    "One way to explore how much a text compares to other texts from its time is to compare the relative frequencies of the terms it uses over and against a corpus of texts: it could be your own corpus, an established corpus, or a large (and wonky) corpus like Google's [Books Ngram Viewer](https://books.google.com/ngrams/).\n",
    "\n",
    "The relative frequency is as easy as dividing a term's count by the overall length of the text. (Note: not the vocabulary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall how we get the count for a term\n",
    "mdg_dict[\"hunter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the math\n",
    "mdg_term_rf = mdg_dict[\"hunter\"] / len(mdg_words)\n",
    "print(mdg_term_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to Google ngram viewer for 1924\n",
    "# Sigh, yes this number was retrieved by hand\n",
    "year_comp = mdg_term_rf / 0.0004010409\n",
    "print(year_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Texts by Comparing Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Up Next:</b> What are the ways we can compare texts?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare term frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare relative term frequencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
